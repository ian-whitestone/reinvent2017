{"paragraphs":[{"text":"%md\n# AWS Glue - Introduction to data preparation\n\nIn this module you will be introduced to using AWS Glue APIs to read, write and manipulate data.  All of the code written in this interactive notebook is compatible with\nthe AWS Glue ETL engine and can be copied into a working ETL script.","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>AWS Glue - Introduction to data preparation</h1>\n<p>In this module you will be introduced to using AWS Glue APIs to read, write and manipulate data. All of the code written in this interactive notebook is compatible with<br/>the AWS Glue ETL engine and can be copied into a working ETL script.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263933_-41033529","id":"20171119-195626_1312727557","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11126"},{"text":"%md\n# Step 1\nLets first create the Spark and Glue context objects required for all jobs.  We will also import Glue transformation functions and the AWS Boto3 SDK","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 1</h1>\n<p>Lets first create the Spark and Glue context objects required for all jobs. We will also import Glue transformation functions and the AWS Boto3 SDK</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263934_-39879282","id":"20171119-195903_1862047042","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11127"},{"text":"from pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.transforms import *\n\nimport boto3\n\n# sc = SparkContext()  # we will not reload the Spark context because our Zeppelin notebook already created one for us\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session","user":"admin","dateUpdated":"2017-11-27T13:48:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511790263934_-39879282","id":"20171119-195424_1999649005","dateCreated":"2017-11-27T13:44:23+0000","dateStarted":"2017-11-27T13:48:16+0000","dateFinished":"2017-11-27T13:48:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11128"},{"text":"%md\n# Step 2\nWe're going to setup a Glue Crawler to autodiscover the [public NYC taxi dataset](http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml) we made available at s3://serverless-analytics/canonical/NY-Pub/\nOnce discovered, the crawler will create a table for us to use.\n\n1. Open the AWS Console and select AWS Glue\n2. Select Crawler from the left hand side and click the __Add Crawler__ button\n3. Give your crawler a name and choose and existing Glue IAM role i.e. __AWSGlueServiceRole__ or choose to create a new one\n4. Select S3 as the Data Source and specify a path in another account. Paste __s3://serverless-analytics/canonical/NY-Pub/__ as the S3 path.\n5. Do not add any additional data sources and select __Run On Demand__ for frequency\n6. Select the __default__ database and give your table a prefix i.e. __taxi___\n7. Click __Finish__ to complete creating the crawler\n8. Run the crawler you created\n\nWhen the crawler completes you should have a table named __taxi_ny_pub__ in your __default__ database.","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 2</h1>\n<p>We&rsquo;re going to setup a Glue Crawler to autodiscover the <a href=\"http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\">public NYC taxi dataset</a> we made available at s3://serverless-analytics/canonical/NY-Pub/<br/>Once discovered, the crawler will create a table for us to use.</p>\n<ol>\n  <li>Open the AWS Console and select AWS Glue</li>\n  <li>Select Crawler from the left hand side and click the <strong>Add Crawler</strong> button</li>\n  <li>Give your crawler a name and choose and existing Glue IAM role i.e. <strong>AWSGlueServiceRole</strong> or choose to create a new one</li>\n  <li>Select S3 as the Data Source and specify a path in another account. Paste <strong>s3://serverless-analytics/canonical/NY-Pub/</strong> as the S3 path.</li>\n  <li>Do not add any additional data sources and select <strong>Run On Demand</strong> for frequency</li>\n  <li>Select the <strong>default</strong> database and give your table a prefix i.e. <strong>taxi</strong>_</li>\n  <li>Click <strong>Finish</strong> to complete creating the crawler</li>\n  <li>Run the crawler you created</li>\n</ol>\n<p>When the crawler completes you should have a table named <strong>taxi_ny_pub</strong> in your <strong>default</strong> database.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263935_-40264031","id":"20171119-200032_1722272843","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11129"},{"text":"%md\n# Step 3\n\nLets now use the Glue DynamicFrame API to read the Taxi data using the Glue Data Catalog.  We'll then print out the table schema as a reference.\nMake sure you substitute the database and table names for what you defined with your Glue Crawler","user":"admin","dateUpdated":"2017-11-27T13:50:29+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 3</h1>\n<p>Lets now use the Glue DynamicFrame API to read the Taxi data using the Glue Data Catalog. We&rsquo;ll then print out the table schema as a reference.<br/>Make sure you substitute the database and table names for what you defined with your Glue Crawler</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263935_-40264031","id":"20171120-002552_72508898","dateCreated":"2017-11-27T13:44:23+0000","dateStarted":"2017-11-27T13:50:29+0000","dateFinished":"2017-11-27T13:50:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11130"},{"text":"taxis = glueContext.create_dynamic_frame.from_catalog(database = <YOUR DB NAME>, table_name = <YOUR TABLE NAME>, transformation_ctx = \"taxis\")\ntaxis.printSchema()","user":"admin","dateUpdated":"2017-11-27T14:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511790263935_-40264031","id":"20171119-200845_139999411","dateCreated":"2017-11-27T13:44:23+0000","dateStarted":"2017-11-27T13:48:47+0000","dateFinished":"2017-11-27T13:48:47+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11131"},{"text":"%md\nFilter out all rides that have a __total_amount__ of $0\nThe Filter API takes as parameters the DynamicFrame with the data, in our case the variable we read the data into.  A function that is called for every row in the dataset to test a condition returning True if the row should be kept or False if the row should be dropped.\n\nGo ahead and complete the filter function below. ","user":"admin","dateUpdated":"2017-11-27T13:47:25+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Filter out all rides that have a <strong>total_amount</strong> of $0<br/>The Filter API takes as parameters the DynamicFrame with the data, in our case the variable we read the data into. A function that is called for every row in the dataset to test a condition returning True if the row should be kept or False if the row should be dropped.</p>\n<p>Go ahead and complete the filter function below.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263936_43995978","id":"20171120-011458_2053263160","dateCreated":"2017-11-27T13:44:23+0000","dateStarted":"2017-11-27T13:47:25+0000","dateFinished":"2017-11-27T13:47:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11132"},{"text":"def filter_function(row):\n  ## complete me\n  \ntaxis_df = Filter.apply(frame = taxis, f = filter_function, transformation_ctx = \"taxis_df\")","user":"admin","dateUpdated":"2017-11-27T14:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511790263936_43995978","id":"20171120-002536_1213331941","dateCreated":"2017-11-27T13:44:23+0000","dateStarted":"2017-11-27T13:48:36+0000","dateFinished":"2017-11-27T13:48:36+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11133"},{"text":"%md\nLets rename the __store_and_fwd_flag__ column and drop the __ratecodeid__, __extra__ and __improvement_surcharge__","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Lets rename the <strong>store_and_fwd_flag</strong> column and drop the <strong>ratecodeid</strong>, <strong>extra</strong> and <strong>improvement_surcharge</strong></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263936_43995978","id":"20171120-013916_349335728","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11134"},{"text":"taxis_df = RenameField.apply(frame = taxis_df, old_name = \"store_and_fwd_flag\", new_name = <NEW NAME>, transformation_ctx = \"taxis_df\")\ntaxis_df = DropFields.apply(frame = taxis_df, paths = [\"ratecodeid\", \"extra\", \"improvement_surcharge\"], transformation_ctx = \"taxis_df\")","dateUpdated":"2017-11-27T14:00:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511790263937_43611229","id":"20171120-010553_58427980","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11135"},{"text":"%md\nLets use the SplitRows API to split the current DynamicFrame on the __payment_type__ column.  Then we'll create two separate DynamicFrames, one for each payment type.","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Lets use the SplitRows API to split the current DynamicFrame on the <strong>payment_type</strong> column. Then we&rsquo;ll create two separate DynamicFrames, one for each payment type.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263938_44765476","id":"20171120-020754_420980253","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11136"},{"text":"df_coll = SplitRows.apply(frame = taxis_df, name1 = 'cc', name2 = 'cash', comparison_dict = {\"payment_type\": {\"=\": 1}}, transformation_ctx = \"df_coll\")","dateUpdated":"2017-11-27T13:44:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511790263938_44765476","id":"20171120-014009_988770561","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11137"},{"text":"creditcard_df = SelectFromCollection.apply(dfc = df_coll, key = 'cc', transformation_ctx = \"creditcard_df\")\ncash_df = SelectFromCollection.apply(dfc = df_coll, key = 'cash', transformation_ctx = \"cash_df\")","dateUpdated":"2017-11-27T13:44:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511790263938_44765476","id":"20171120-015732_449432221","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11138"},{"text":"%md\nNext we want to use SQL to look at the data so we need to convert the DynamicFrame to a Spark DataFrame and register it as a temporary view","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Next we want to use SQL to look at the data so we need to convert the DynamicFrame to a Spark DataFrame and register it as a temporary view</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263938_44765476","id":"20171120-022046_496821941","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11139"},{"text":"creditcard_df.toDF().createOrReplaceTempView('rides')","dateUpdated":"2017-11-27T13:44:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511790263939_44380727","id":"20171120-020432_1137478616","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11140"},{"text":"%md\n# Step 4\nGo ahead and use SQL to explore the data.  Try graphing the data using Zeppelin's built in graphs.","dateUpdated":"2017-11-27T13:44:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 4</h1>\n<p>Go ahead and use SQL to explore the data. Try graphing the data using Zeppelin&rsquo;s built in graphs.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790263939_44380727","id":"20171120-050958_2039223691","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11141"},{"text":"%sql\nselect date_format(tpep_dropoff_datetime, 'yyyy-MM-dd') as dt, sum(trip_distance), sum(total_amount)\nfrom rides\ngroup by date_format(tpep_dropoff_datetime, 'yyyy-MM-dd')\norder by dt","dateUpdated":"2017-11-27T13:44:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"scatterChart":{"xAxis":{"name":"trip_distance","index":0,"aggr":"sum"},"yAxis":{"name":"total_amount","index":1,"aggr":"sum"},"group":{"name":"passenger_count","index":2,"aggr":"sum"}},"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"trip_distance","index":0,"aggr":"sum"}],"groups":[{"name":"passenger_count","index":2,"aggr":"sum"}],"values":[{"name":"total_amount","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"dt\tsum(trip_distance)\tsum(total_amount)\n2016-01-01\t630467.7399999923\t3199045.949997261\n2016-01-02\t574058.3799999951\t2961452.440001506\n2016-01-03\t672242.6299999988\t3308236.7300012554\n2016-01-04\t636568.6900000015\t3361621.41000196\n2016-01-05\t656724.6700000023\t3663479.2400020594\n2016-01-06\t663543.5200000041\t3746478.880002026\n2016-01-07\t711749.3699999976\t4031341.840002285\n2016-01-08\t738704.7999999959\t4246054.740001872\n2016-01-09\t737659.3100000019\t4093752.580002579\n2016-01-10\t752247.1800000026\t3870561.0700023333\n2016-01-11\t702768.9399999969\t3903051.5100008463\n2016-01-12\t721648.0700000069\t4123705.050001476\n2016-01-13\t772513.439999998\t4475139.860002442\n2016-01-14\t820689.4100000052\t4677331.550002781\n2016-01-15\t840450.5099999983\t4728453.410001081\n2016-01-16\t784926.6699999948\t4277600.500001344\n2016-01-17\t765348.0799999972\t4058856.180002099\n2016-01-18\t709037.6099999958\t3685261.060001217\n2016-01-19\t781063.6800000047\t4524687.9299976835\n2016-01-20\t794550.7500000035\t4579708.159999715\n2016-01-21\t817033.6200000057\t4799046.860001009\n2016-01-22\t819841.89\t4759334.720001328\n2016-01-23\t160086.15000000165\t893500.1300002645\n2016-01-24\t304057.6700000016\t1653508.000001039\n2016-01-25\t643178.1999999987\t3799569.0800016695\n2016-01-26\t723510.0000000023\t4425981.720002142\n2016-01-27\t746291.8900000004\t4545318.6900019795\n2016-01-28\t5473705.290000006\t4786541.490001786\n2016-01-29\t828907.9900000014\t4897930.529998239\n2016-01-30\t802472.6999999948\t4601403.219994234\n2016-01-31\t792680.6099999805\t4176877.4199959394\n2016-02-01\t10075.459999999994\t42303.39000000021\n2016-02-02\t17.5\t75.39\n2016-02-05\t17.4\t72.84\n2016-02-06\t5.0\t23.8\n2016-02-11\t0.7\t6.96\n2016-02-12\t15.5\t69.99\n2016-03-01\t18.6\t63.35\n2016-03-28\t2.1\t14.75\n"}]},"apps":[],"jobName":"paragraph_1511790263939_44380727","id":"20171120-021231_951231394","dateCreated":"2017-11-27T13:44:23+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11142"},{"text":"%md\nCan you rewrite the above query but this name give the summation columns proper names and format the values in a more friendly way ?\nHere is a [hint](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.format_number)\n\nFeel free to explore the data using SQL","user":"admin","dateUpdated":"2017-11-27T14:38:05+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Can you rewrite the above query but this name give the summation columns proper names and format the values in a more friendly way ?<br/>Here is a <a href=\"http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.format_number\">hint</a></p>\n<p>Feel free to explore the data using SQL</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511790775274_-1500647739","id":"20171127-135255_56171427","dateCreated":"2017-11-27T13:52:55+0000","dateStarted":"2017-11-27T14:38:05+0000","dateFinished":"2017-11-27T14:38:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11143"},{"text":"%md\n# Finished!\n\nYou are now finished with the first module  Thank you.","dateUpdated":"2017-11-27T14:37:23+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511790263940_42456983","id":"20171120-022039_263702524","dateCreated":"2017-11-27T13:44:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11144","user":"admin","dateFinished":"2017-11-27T14:37:23+0000","dateStarted":"2017-11-27T14:37:23+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Finished!</h1>\n<p>You are now finished with the first module Thank you.</p>\n</div>"}]}},{"text":"%md\n","user":"admin","dateUpdated":"2017-11-27T14:37:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511793442628_677046884","id":"20171127-143722_1399536797","dateCreated":"2017-11-27T14:37:22+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:12342"}],"name":"Workshop 1","id":"2CZM5JJKS","angularObjects":{"2CRE5X63N:shared_process":[],"2CQZ9JC8F:shared_process":[],"2CNAPD13W:shared_process":[],"2CRC324G8:existing_process":[],"2CQ1X8PFU:shared_process":[],"2CNFC7TNE:shared_process":[],"2CMQ2EHTE:shared_process":[],"2CQUCJVJA:shared_process":[],"2CQY3SUDK:shared_process":[],"2CNA37GFQ:shared_process":[],"2CPH9H73W:shared_process":[],"2CR2XFEE3:shared_process":[],"2CQ9YVF7W:shared_process":[],"2CP1P36EU:shared_process":[],"2CMR9MKCE:shared_process":[],"2CR2YZBFW:shared_process":[],"2CPJK4U6W:shared_process":[],"2CR2Z9UR2:shared_process":[],"2CPQWHWM3:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}