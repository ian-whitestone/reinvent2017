{"paragraphs":[{"text":"%md\n# Parsing web forums using AWS Glue\nIn this excercise we will write a simple web crawler to collect AWS forum posts, clean and structure them.\nWe will then preprocess the data further to prepare it for machine leaning.\nSubsequently we will build a simple Word2Vec model and an LDA clustering model with our data","dateUpdated":"2017-11-27T14:16:08+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470560_959865806","id":"20171119-160246_991167287","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2528","user":"admin","dateFinished":"2017-11-27T14:16:08+0000","dateStarted":"2017-11-27T14:16:08+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Parsing web forums using AWS Glue</h1>\n<p>In this excercise we will write a simple web crawler to collect AWS forum posts, clean and structure them.<br/>We will then preprocess the data further to prepare it for machine leaning.<br/>Subsequently we will build a simple Word2Vec model and an LDA clustering model with our data</p>\n</div>"}]}},{"text":"%md\n## Step 1\nLoad a package of modules needed for this excercise.  Including Requests, BeautifulSoup and Numpy (needed by SparkML)\nIn the following cell we will import the required modules","dateUpdated":"2017-11-27T14:16:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470560_959865806","id":"20171119-160808_1875546943","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2529","user":"admin","dateFinished":"2017-11-27T14:16:19+0000","dateStarted":"2017-11-27T14:16:19+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Step 1</h2>\n<p>Load a package of modules needed for this excercise. Including Requests, BeautifulSoup and Numpy (needed by SparkML)<br/>In the following cell we will import the required modules</p>\n</div>"}]}},{"text":"sc.addPyFile('s3://workshop-public/packages/web_parsing_modules.zip')","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470560_959865806","id":"20171115-004336_529487646","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2530"},{"text":"from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom collections import OrderedDict\nfrom bs4 import BeautifulSoup\nimport urlparse as url\nimport requests\nimport numpy\nimport json\nimport sys\nimport re","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470560_959865806","id":"20171115-004421_1144536098","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2531"},{"text":"%md\n# Step 2\nFirst cell will include variables to hold paths to our AWS forums that we want to parse\nSecond cell include helper functions used to iterate through the forum posts and extract the relavent information","dateUpdated":"2017-11-27T14:16:27+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470561_959481057","id":"20171119-160948_1313809756","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2532","user":"admin","dateFinished":"2017-11-27T14:16:27+0000","dateStarted":"2017-11-27T14:16:27+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 2</h1>\n<p>First cell will include variables to hold paths to our AWS forums that we want to parse<br/>Second cell include helper functions used to iterate through the forum posts and extract the relavent information</p>\n</div>"}]}},{"text":"AWSFORUMS = 'https://forums.aws.amazon.com/'\nATHENA_FORUM = 'forum.jspa?forumID=242&start=0'","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470561_959481057","id":"20171115-004427_135993240","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2533"},{"text":"def listPosts(items):\n  posts = []\n  for item in items:\n    post = {}\n    # thread subject line and link\n    i = item.find('a', id='jive-thread-0')\n    post['subject'] = i.string\n    post['link'] = AWSFORUMS + i['href']\n\n    # give our post a unique id for tracking purposes\n    path = url.urlparse(post['link'])\n    post['id'] = url.parse_qs(path.query)['threadID'][0]\n  \n    # thread view and reply count\n    i = item.find('td', 'jive-view-count').string.split()\n    if len(i) == 0:\n      post['views'] = 0\n      post['replies'] = 0\n    else:\n      post['views'] = i[0]\n      post['replies'] = i[2]\n\n    posts.append(post)\n\n  return posts\n  \ndef getPostContent(post):\n  data = requests.get(post['link'])\n  html = BeautifulSoup(data.text, 'html.parser')\n  \n  post['date'] = html.select('div.jive-message-info font')[0].string\n  body = []\n\n  messages = html.select('div.jive-message-body')\n  if len(messages) > 0:\n    for msg in messages:\n  \t  body.append(list(msg.div.stripped_strings))\n  \n  post['body'] = [val for sublist in body for val in sublist] # flatten the nested list of lists\n  return post","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470561_959481057","id":"20171115-004508_1030813136","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2534"},{"text":"%md\n# Step 3\nIn this cell we will setup a simple thread pool that will allow us to execute multiple concurrent GET requests to the forums URL.\nOnce we get the page we will parse it with BeautifulSoup and use [CSS selectors](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to extracts the bits we want.\nLastly we will JSON encode each post after it was created so we can easily convert it into a tabular representation known as DataFrame in Spark","dateUpdated":"2017-11-27T14:16:32+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470561_959481057","id":"20171119-161126_686858478","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2535","user":"admin","dateFinished":"2017-11-27T14:16:32+0000","dateStarted":"2017-11-27T14:16:32+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 3</h1>\n<p>In this cell we will setup a simple thread pool that will allow us to execute multiple concurrent GET requests to the forums URL.<br/>Once we get the page we will parse it with BeautifulSoup and use <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\">CSS selectors</a> to extracts the bits we want.<br/>Lastly we will JSON encode each post after it was created so we can easily convert it into a tabular representation known as DataFrame in Spark</p>\n</div>"}]}},{"text":"pool = ThreadPoolExecutor(2)\n\nforum = requests.get(AWSFORUMS + ATHENA_FORUM)\nhtml = BeautifulSoup(forum.text, 'html.parser')\nitems = html.select('.jive-thread-list > .jive-table tbody > tr')\n\nposts = listPosts(items)\n  \nfutures = [pool.submit(getPostContent, post) for post in posts]\n\njson_posts = []\n\nfor r in as_completed(futures):\n  try:\n    json_posts.append(json.dumps(r.result()))\n  except Exception, err:\n    sys.stderr.write('*** ERROR: %sn' % str(err))\n\n\nrdd = sc.parallelize(json_posts)\ndf = spark.read.json(rdd)","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470562_960635303","id":"20171115-004526_1266005733","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2536"},{"text":"%md\n# Step 4\nNext we will import additional modules to use with further data prep.\nCreate a simple Python lambda expression to be used for further removing unwanted characters from our post text","dateUpdated":"2017-11-27T14:16:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470562_960635303","id":"20171119-161559_1131108117","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2537","user":"admin","dateFinished":"2017-11-27T14:16:37+0000","dateStarted":"2017-11-27T14:16:37+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 4</h1>\n<p>Next we will import additional modules to use with further data prep.<br/>Create a simple Python lambda expression to be used for further removing unwanted characters from our post text</p>\n</div>"}]}},{"text":"from pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.sql.types import StringType, ArrayType, DoubleType\nfrom pyspark.sql.functions import udf","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470562_960635303","id":"20171115-020704_437517382","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2538"},{"text":"%md\nThe following cell prepares two things:\n1. Creates a Regular Expression that keeps only letters, digits and spaces.\n2. Register a User Defined Function that will be given the post body from each row in the dataset, join the multiple array of strings into a single array of sentences\n\nNext we call three actions:\n1. Use Spark to take our dataset and distribute it across the entire cluster of nodes for better parallel processing\n2. Parse the datset as JSON and create a tabluar representation known as DataFrame\n3. Append a new column called __sentences__ made up of the cleaned post body using the UDF we declared\n\nNote that the last line is incomplete and you will need to add the correct code.  Hint, re-read #3 above.","user":"admin","dateUpdated":"2017-11-27T14:16:44+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470562_960635303","id":"20171119-162430_543435773","dateCreated":"2017-11-27T14:04:30+0000","dateStarted":"2017-11-27T14:16:44+0000","dateFinished":"2017-11-27T14:16:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2539","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The following cell prepares two things:<br/>1. Creates a Regular Expression that keeps only letters, digits and spaces.<br/>2. Register a User Defined Function that will be given the post body from each row in the dataset, join the multiple array of strings into a single array of sentences</p>\n<p>Next we call three actions:<br/>1. Use Spark to take our dataset and distribute it across the entire cluster of nodes for better parallel processing<br/>2. Parse the datset as JSON and create a tabluar representation known as DataFrame<br/>3. Append a new column called <strong>sentences</strong> made up of the cleaned post body using the UDF we declared</p>\n<p>Note that the last line is incomplete and you will need to add the correct code. Hint, re-read #3 above.</p>\n</div>"}]}},{"text":"removeExp = re.compile(r'[^\\w\\d\\s]|\\n|\\r')\ncleanUdf = udf(lambda s: re.sub(removeExp, '', ' '.join(s)), StringType())\n\nrdd = sc.parallelize(json_posts)\ndf = spark.read.json(rdd)\ndf = df.withColumn('sentences', cleanUdf(df.body))","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470562_960635303","id":"20171117-140642_214287606","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2540"},{"text":"%md\nPrinting out the [schema](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema) of our DataFrame lets us see the column names and types","user":"admin","dateUpdated":"2017-11-27T14:16:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470563_960250554","id":"20171119-164939_447996916","dateCreated":"2017-11-27T14:04:30+0000","dateStarted":"2017-11-27T14:16:48+0000","dateFinished":"2017-11-27T14:16:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2541","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Printing out the <a href=\"http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema\">schema</a> of our DataFrame lets us see the column names and types</p>\n</div>"}]}},{"text":"## Print out the schema","user":"admin","dateUpdated":"2017-11-27T14:10:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470563_960250554","id":"20171117-141226_1470180729","dateCreated":"2017-11-27T14:04:30+0000","dateStarted":"2017-11-27T14:10:58+0000","dateFinished":"2017-11-27T14:14:20+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2542"},{"text":"%md\n# Step 5\nThe next cell uses two functions from the Spark MLlib library to first tokenize (split into words) the post text and second to remove step words.\nThis step is most common in text processing workflows","dateUpdated":"2017-11-27T14:16:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470563_960250554","id":"20171119-165009_46148430","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2543","user":"admin","dateFinished":"2017-11-27T14:16:55+0000","dateStarted":"2017-11-27T14:16:55+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 5</h1>\n<p>The next cell uses two functions from the Spark MLlib library to first tokenize (split into words) the post text and second to remove step words.<br/>This step is most common in text processing workflows</p>\n</div>"}]}},{"text":"tokenizer = Tokenizer(inputCol=\"sentences\", outputCol=\"words\")\nwordsData = tokenizer.transform(df)\n\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filteredWords\")\nfilteredData = remover.transform(wordsData)","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470563_960250554","id":"20171117-133607_1440074263","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2544"},{"text":"%md\n# Step 6\nIn the next cell we use SparkML's [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) implementation to generate word embeddings based on our posts' text.\nWord2Vec is an interesting algorithm that creates numerical representations of words (vector aka. array of numbers) that are mapped across a large vector space based on word\nsimilarities identified from the corpus of documents. When searching for word similarities using the __findSynonyms__ function, the algorithm searches the entire vector space\nto match words that are most similar based on their relation to the rest of the corpus.\n\nIt's important to keep in mind that we only have a very small subset of text to train the model on so the results will not be very good.  But still worth understading how to use.","dateUpdated":"2017-11-27T14:17:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470564_958326810","id":"20171119-165148_1892706580","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2545","user":"admin","dateFinished":"2017-11-27T14:17:02+0000","dateStarted":"2017-11-27T14:17:02+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 6</h1>\n<p>In the next cell we use SparkML&rsquo;s <a href=\"https://en.wikipedia.org/wiki/Word2vec\">Word2Vec</a> implementation to generate word embeddings based on our posts&rsquo; text.<br/>Word2Vec is an interesting algorithm that creates numerical representations of words (vector aka. array of numbers) that are mapped across a large vector space based on word<br/>similarities identified from the corpus of documents. When searching for word similarities using the <strong>findSynonyms</strong> function, the algorithm searches the entire vector space<br/>to match words that are most similar based on their relation to the rest of the corpus.</p>\n<p>It&rsquo;s important to keep in mind that we only have a very small subset of text to train the model on so the results will not be very good. But still worth understading how to use.</p>\n</div>"}]}},{"text":"from pyspark.ml.feature import Word2Vec\nword2Vec = Word2Vec(vectorSize=5, seed=42, inputCol=\"filteredWords\", outputCol=\"word_vec\")\nwv_model = word2Vec.fit(filteredData)\nwv_model.findSynonyms('query', 2).show()","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470564_958326810","id":"20171117-141454_493666381","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2546"},{"text":"%md\n# Step 7\n[Latent Dirichlet allocation (LDA)](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) is an algorithm similar in function to clustering which is used to identify numberic topics in text based on relationships of words in that text.\n\nWe first need to run a CountVectorizer which creates a vector of word counts per document (forum post) which is then fed into the LDA algorithm.\nWe then fit (train) the model on the data and transform it on the same data.  Again we don't have a lot of data to train on and you don't want to train and predict on the same data.\n\nLastly we take the result DataFrame and create a temporary view so we can use SQL to analyze it.","dateUpdated":"2017-11-27T14:17:06+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470564_958326810","id":"20171119-170038_1726501505","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2547","user":"admin","dateFinished":"2017-11-27T14:17:06+0000","dateStarted":"2017-11-27T14:17:06+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 7</h1>\n<p><a href=\"http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\">Latent Dirichlet allocation (LDA)</a> is an algorithm similar in function to clustering which is used to identify numberic topics in text based on relationships of words in that text.</p>\n<p>We first need to run a CountVectorizer which creates a vector of word counts per document (forum post) which is then fed into the LDA algorithm.<br/>We then fit (train) the model on the data and transform it on the same data. Again we don&rsquo;t have a lot of data to train on and you don&rsquo;t want to train and predict on the same data.</p>\n<p>Lastly we take the result DataFrame and create a temporary view so we can use SQL to analyze it.</p>\n</div>"}]}},{"text":"from pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.clustering import LDA\n\ncv = CountVectorizer().setInputCol('filteredWords').setOutputCol('features').setVocabSize(500).setBinary(True)\ncounted_data = cv.fit(filteredData).transform(filteredData)\nlda = LDA().setK(3).setMaxIter(10).setOptimizer('em')\nlda_model = lda.fit(counted_data)\nlda_topics_df = lda_model.transform(counted_data).select('subject', 'topicDistribution')\n\nlda_topics_df.createOrReplaceTempView('topics')\n\n# The topic weights are in a vector format so we need to convert them. We'll register a UDF to do that with SQL\nspark.udf.register('toArrayUdf', (lambda r: r.toArray().tolist()), ArrayType(DoubleType()))\n\n# A UDF to extract the index in the topics array for which the value is the largest.  This identifys which topic the post is most closely realted to.\ndef maxUdf(items):\n  l = items.toArray().tolist()\n  return l.index(max(l))\nspark.udf.register('toMaxUdf', maxUdf, IntegerType())","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470564_958326810","id":"20171117-145901_1405390061","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2548"},{"text":"%md\nAs you can see from the table below, for each post the LDA model created 3 clusters and identified the impact of the post on each cluster.","dateUpdated":"2017-11-27T14:17:11+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470564_958326810","id":"20171119-173803_1665999243","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2549","user":"admin","dateFinished":"2017-11-27T14:17:11+0000","dateStarted":"2017-11-27T14:17:11+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As you can see from the table below, for each post the LDA model created 3 clusters and identified the impact of the post on each cluster.</p>\n</div>"}]}},{"text":"%sql\nwith dataset as (select subject, toArrayUdf(topicDistribution) as weights from topics)\nselect subject, weights[0] as k1, weights[1] as k2, weights[2] as k3\nfrom dataset","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{},"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470565_957942061","id":"20171119-180119_639323755","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2550"},{"text":"%md\nLets visualize the topic distribution as it relates to the total number of posts most related to those topics based on the max value of their weights.\n","dateUpdated":"2017-11-27T14:17:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470565_957942061","id":"20171119-193729_393961912","dateCreated":"2017-11-27T14:04:30+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2551","user":"admin","dateFinished":"2017-11-27T14:17:15+0000","dateStarted":"2017-11-27T14:17:15+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Lets visualize the topic distribution as it relates to the total number of posts most related to those topics based on the max value of their weights.</p>\n</div>"}]}},{"text":"%sql\nselect subject, toMaxUdf(topicDistribution) as topic\nfrom topics","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":false,"setting":{"scatterChart":{"xAxis":{"name":"subject","index":0,"aggr":"sum"},"yAxis":{"name":"topic","index":1,"aggr":"sum"}},"multiBarChart":{},"pieChart":{}},"keys":[],"groups":[{"name":"topic","index":1,"aggr":"sum"}],"values":[{"name":"topic","index":1,"aggr":"count"}],"commonSetting":{}},"helium":{}}},"enabled":true,"editorSetting":{"language":"sql","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470565_957942061","id":"20171119-153038_2124498598","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2552"},{"text":"%md\n# Finished!\nYou are now finished with this module.  Feel free to continue exploring or continue to the next module.","user":"admin","dateUpdated":"2017-11-27T14:17:20+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470566_959096308","id":"20171119-180817_671322318","dateCreated":"2017-11-27T14:04:30+0000","dateStarted":"2017-11-27T14:17:20+0000","dateFinished":"2017-11-27T14:17:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2553","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Finished!</h1>\n<p>You are now finished with this module. Feel free to continue exploring or continue to the next module.</p>\n</div>"}]}},{"text":"%md\n","dateUpdated":"2017-11-27T14:04:30+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511791470566_959096308","id":"20171119-194705_1190359242","dateCreated":"2017-11-27T14:04:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2554"}],"name":"Workshop 2","id":"2D2EA2UEK","angularObjects":{"2CRE5X63N:shared_process":[],"2CQZ9JC8F:shared_process":[],"2CNAPD13W:shared_process":[],"2CRC324G8:existing_process":[],"2CQ1X8PFU:shared_process":[],"2CNFC7TNE:shared_process":[],"2CMQ2EHTE:shared_process":[],"2CQUCJVJA:shared_process":[],"2CQY3SUDK:shared_process":[],"2CNA37GFQ:shared_process":[],"2CPH9H73W:shared_process":[],"2CR2XFEE3:shared_process":[],"2CQ9YVF7W:shared_process":[],"2CP1P36EU:shared_process":[],"2CMR9MKCE:shared_process":[],"2CR2YZBFW:shared_process":[],"2CPJK4U6W:shared_process":[],"2CR2Z9UR2:shared_process":[],"2CPQWHWM3:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}