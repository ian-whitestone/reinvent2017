{"paragraphs":[{"text":"%md\n# Graph Relationship\n\nIn this module we'll be using data from [UMN/Sarwat Foursquare Dataset](https://archive.org/details/201309_foursquare_dataset_umn) which consists of 2153471 users, 1143092 venues, 1021970 check-ins, 27098490 social connections, and 2809581 ratings that users assigned to venues; all extracted from the Foursquare application through the public API. All users information have been anonymized, i.e., users geolocations are also anonymized. We will learn how to read the data, clean it and use [GraphFrames](https://graphframes.github.io/index.html) to build a graph, search it and find connections.\n\n## Content of Files\n  * users.dat: consists of a set of users such that each user has a unique id and a geospatial location (latitude and longitude) that represents the user home town location.\n  * venues.dat: consists of a set of venues (e.g., restaurants) such that each venue has a unique id and a geospatial location (lattude and longitude).\n  * checkins.dat: marks the checkins (visits) of users at venues. Each check-in has a unique id as well as the user id and the venue id.\n  * socialgraph.dat: contains the social graph edges (connections) that exist between users. Each social connection consits of two users (friends) represented by two unique ids (first_user_id and second_user_id).\n  * ratings.dat: consists of implicit ratings that quantifies how much a user likes a specific venue.\n\n## Credits\n  * Mohamed Sarwat, Justin J. Levandoski, Ahmed Eldawy, and Mohamed F. Mokbel. LARS*: A Scalable and Efficient Location-Aware Recommender System. in IEEE Transactions on Knowledge and Data Engineering TKDE\n  * Justin J. Levandoski, Mohamed Sarwat, Ahmed Eldawy, and Mohamed F. Mokbel. LARS: A Location-Aware Recommender System. in ICDE 2012","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Graph Relationship</h1>\n<p>In this module we&rsquo;ll be using data from <a href=\"https://archive.org/details/201309_foursquare_dataset_umn\">UMN/Sarwat Foursquare Dataset</a> which consists of 2153471 users, 1143092 venues, 1021970 check-ins, 27098490 social connections, and 2809581 ratings that users assigned to venues; all extracted from the Foursquare application through the public API. All users information have been anonymized, i.e., users geolocations are also anonymized. We will learn how to read the data, clean it and use <a href=\"https://graphframes.github.io/index.html\">GraphFrames</a> to build a graph, search it and find connections.</p>\n<h2>Content of Files</h2>\n<ul>\n  <li>users.dat: consists of a set of users such that each user has a unique id and a geospatial location (latitude and longitude) that represents the user home town location.</li>\n  <li>venues.dat: consists of a set of venues (e.g., restaurants) such that each venue has a unique id and a geospatial location (lattude and longitude).</li>\n  <li>checkins.dat: marks the checkins (visits) of users at venues. Each check-in has a unique id as well as the user id and the venue id.</li>\n  <li>socialgraph.dat: contains the social graph edges (connections) that exist between users. Each social connection consits of two users (friends) represented by two unique ids (first_user_id and second_user_id).</li>\n  <li>ratings.dat: consists of implicit ratings that quantifies how much a user likes a specific venue.</li>\n</ul>\n<h2>Credits</h2>\n<ul>\n  <li>Mohamed Sarwat, Justin J. Levandoski, Ahmed Eldawy, and Mohamed F. Mokbel. LARS*: A Scalable and Efficient Location-Aware Recommender System. in IEEE Transactions on Knowledge and Data Engineering TKDE</li>\n  <li>Justin J. Levandoski, Mohamed Sarwat, Ahmed Eldawy, and Mohamed F. Mokbel. LARS: A Location-Aware Recommender System. in ICDE 2012</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381759_1403363628","id":"20171124-004046_381461252","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5073"},{"text":"%md\n# Step 1\n\nIn this step we're using Zeppelin's feature for loading external modules from an MVN repository.  You **must** run this cell first and allow it to complete before running other cells.\nOnce completed run the next cell and allow it to also complete before running the other cells.\n\nThe reason we need to run these cells first is because they load external modules into the Spark Context and that needs to happen before it is used in our program","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 1</h1>\n<p>In this step we&rsquo;re using Zeppelin&rsquo;s feature for loading external modules from an MVN repository. You <strong>must</strong> run this cell first and allow it to complete before running other cells.<br/>Once completed run the next cell and allow it to also complete before running the other cells.</p>\n<p>The reason we need to run these cells first is because they load external modules into the Spark Context and that needs to happen before it is used in our program</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381760_1389127918","id":"20171124-004540_48556387","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5074"},{"text":"%spark.dep\nz.load(\"graphframes:graphframes:0.5.0-spark2.1-s_2.11\")","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Must be used before SparkInterpreter (%spark) initialized\nHint: put this paragraph before any Spark code and restart Zeppelin/Interpreter"}]},"apps":[],"jobName":"paragraph_1511792381760_1389127918","id":"20171123-040516_165091080","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5075"},{"text":"## There is one small issue with the GraphFrame library in that it doesn't properly expose its Python module for use in PySpark.\n## To fix that I had to extract the Python files from the JAR and make them available as a separate module.\nsc.addPyFile('s3://workshop-public/packages/graphframes.zip')","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381760_1389127918","id":"20171123-040520_2004241001","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5076"},{"text":"import re\nfrom graphframe import *\nfrom pyspark.sql.functions import *","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381761_1388743169","id":"20171123-013119_292500578","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5077"},{"text":"%md\n# Step 2\nIn this step we will read in all of the data files.  Note that we're not using a convinience function such as read.csv or read.json.  The reason for that is because these files not in a very machine friendly format. The [textFile](http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile) API allows us to read text file and will automatically create a new row for every line assuming it is seperated by a newline character.","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 2</h1>\n<p>In this step we will read in all of the data files. Note that we&rsquo;re not using a convinience function such as read.csv or read.json. The reason for that is because these files not in a very machine friendly format. The <a href=\"http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.SparkContext.textFile\">textFile</a> API allows us to read text file and will automatically create a new row for every line assuming it is seperated by a newline character.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381761_1388743169","id":"20171124-004955_2101000587","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5078"},{"text":"users = sc.textFile('s3://workshop-public/4square/users.dat')\ncheckins = sc.textFile('s3://workshop-public/4square/checkins.dat')\nratings = sc.textFile('s3://workshop-public/4square/ratings.dat')\nvenues = sc.textFile('s3://workshop-public/4square/venues.dat')\nconnections = sc.textFile('s3://workshop-public/4square/socialgraph.dat')","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381761_1388743169","id":"20171122-233540_2112145042","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5079"},{"text":"%md\n# Step 3\n\nThis is where the bulk of the data preparation takes place.  To set up we will create a Regular Expression pattern that  will be used to remove unwanted characters from the data.\n\nNext lets use the Spark API to iterate over the data files one at a time.  The operations are as follows:\n  * map over each row and split the text by the | (pipe) delimiter\n  * map over each split row and remove unwanted characters using the regular expression\n  * filter the result set to only those rows with data in them\n\nOnce we iterate over the data and clean it, we can convert it to a Spark DataFrame, assigning it column names in the process.\n\nIn order to work with GraphFrames, I changed the column names a bit from what the files original used.  The **id** column in all of the DataFrames refers to the user ID.\nThe connections DataFrame holds the social graph and lists the origin user id and the target user id represeting connections between users.  GraphFrames require these two columns to be names **src** for the origin user id and **dst** for the target user id.","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 3</h1>\n<p>This is where the bulk of the data preparation takes place. To set up we will create a Regular Expression pattern that will be used to remove unwanted characters from the data.</p>\n<p>Next lets use the Spark API to iterate over the data files one at a time. The operations are as follows:<br/> * map over each row and split the text by the | (pipe) delimiter<br/> * map over each split row and remove unwanted characters using the regular expression<br/> * filter the result set to only those rows with data in them</p>\n<p>Once we iterate over the data and clean it, we can convert it to a Spark DataFrame, assigning it column names in the process.</p>\n<p>In order to work with GraphFrames, I changed the column names a bit from what the files original used. The <strong>id</strong> column in all of the DataFrames refers to the user ID.<br/>The connections DataFrame holds the social graph and lists the origin user id and the target user id represeting connections between users. GraphFrames require these two columns to be names <strong>src</strong> for the origin user id and <strong>dst</strong> for the target user id.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381761_1388743169","id":"20171124-005406_1284566388","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5080"},{"text":"removeExp = re.compile(r'-?[a-z\\+\\s\\_]+|-{2,}')\n\nu = users \\\n  .map(lambda r: r.split('|')) \\\n  .map(lambda r: map(lambda i: re.sub(removeExp, '', i), r)) \\\n  .filter(lambda r: r[0] != '')\n\ndf_users = spark.createDataFrame(u, ['id', 'user_lat', 'user_lon'])\n\nc = checkins \\\n  <COMPLETE THE FUNCTION>\n  \ndf_checkins = spark.createDataFrame(c, ['checkinid', 'id', 'venueid', 'checkin_lat', 'checkin_lon', 'created'])\n\nr = ratings \\\n  <COMPLETE THE FUNCTION>\n  \ndf_ratings = spark.createDataFrame(r, ['id', 'venueid', 'score'])\n\nv = venues \\\n  <COMPLETE THE FUNCTION>\n  \ndf_venues = spark.createDataFrame(v, ['venueid', 'venue_lat', 'venue_lon'])\n\ncon = connections \\\n  <COMPLETE THE FUNCTION>\n  \ndf_connections = spark.createDataFrame(con, ['src', 'dst'])","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381761_1388743169","id":"20171122-233620_1175154595","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5081"},{"text":"%md\n# Step 4\n\nSince we have multiple files that are related we should join them together to create a comprehensive DataFrame.  Lets first join venues and ratings, then we'll join that DataFrame with the checkins data. Note that we will need to drop the redundant columns.","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 4</h1>\n<p>Since we have multiple files that are related we should join them together to create a comprehensive DataFrame. Lets first join venues and ratings, then we&rsquo;ll join that DataFrame with the checkins data. Note that we will need to drop the redundant columns.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381762_1389897416","id":"20171124-011415_95629213","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5082"},{"text":"venues_ratings = df_venues.join(df_ratings, df_ratings.venueid == df_venues.venueid) \\\n  .drop(df_ratings.venueid)\n  \nvenues_ratings = venues_ratings.selectExpr(\"\"\"\n  CASE WHEN score<=2.0 THEN 'poor'\n  WHEN score>=3.0 AND score<=4.0 THEN 'good'\n  WHEN score>4.0 THEN 'great'\n  END as rating\n\"\"\", \"*\")\n\nvenues_ratings.printSchema()\nvenues_ratings.show(5)","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381762_1389897416","id":"20171123-132412_1865524617","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5083"},{"text":"%md\n# Step 5\n\nNow we will create the GraphFrame from the data.  The GraphFrame constructor takes vertices and edges as parameters.  Vertices are the nodes in the graph and the edges are the connections or paths between these nodes.\n\nIn the next cell we will take a look at the vertices and edges so you can get a feel for what this means.\n\nPrint out a sample of 5 edges and 5 vertices so that we can see what they look like. \nHint: the **graph** object has edges and [vertices](https://graphframes.github.io/api/python/graphframes.html#graphframes.GraphFrame.vertices) elements that are DataFrames.  \nRemember how we show values of a DataFrame ?","dateUpdated":"2017-11-27T14:25:24+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 5</h1>\n<p>Now we will create the GraphFrame from the data. The GraphFrame constructor takes vertices and edges as parameters. Vertices are the nodes in the graph and the edges are the connections or paths between these nodes.</p>\n<p>In the next cell we will take a look at the vertices and edges so you can get a feel for what this means.</p>\n<p>Print out a sample of 5 edges and 5 vertices so that we can see what they look like.<br/>Hint: the <strong>graph</strong> object has edges and <a href=\"https://graphframes.github.io/api/python/graphframes.html#graphframes.GraphFrame.vertices\">vertices</a> elements that are DataFrames.<br/>Remember how we show values of a DataFrame ?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381762_1389897416","id":"20171124-013201_1290588922","dateCreated":"2017-11-27T14:19:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5084","user":"admin","dateFinished":"2017-11-27T14:25:24+0000","dateStarted":"2017-11-27T14:25:24+0000"},{"text":"vertices = venues_ratings.drop('venue_lat').drop('venue_lon')\n\ngraph = GraphFrame(vertices ,df_connections)\n\n## Show the edges and vertices","dateUpdated":"2017-11-27T14:24:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381762_1389897416","id":"20171123-042634_1334262599","dateCreated":"2017-11-27T14:19:41+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5085","user":"admin","dateFinished":"2017-11-27T14:25:00+0000","dateStarted":"2017-11-27T14:25:00+0000"},{"text":"%md\n# Step 6\n\nMotif searching is a method for finding patterns in the graph.  GraphFrame's motif API uses Domain-Specific Language to express structural queries.  You can read the API documentation [here](https://graphframes.github.io/user-guide.html#motif-finding).\n\nFor example, the motif find operation below is searching for any two vertices (nodes) **a** and **b** that are connected by edges in **both** directions.  Kind of like I'm your friend and you are my friend.  Note that the names used in the search pattern are just labels and do not represent the actual data in the GraphFrame.\n\nNote, motif searching is a resource intensive operation.  Glue dev endpoints are not typically provisioned with much capacity so expect this task to take a little bit longer.\n","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Step 6</h1>\n<p>Motif searching is a method for finding patterns in the graph. GraphFrame&rsquo;s motif API uses Domain-Specific Language to express structural queries. You can read the API documentation <a href=\"https://graphframes.github.io/user-guide.html#motif-finding\">here</a>.</p>\n<p>For example, the motif find operation below is searching for any two vertices (nodes) <strong>a</strong> and <strong>b</strong> that are connected by edges in <strong>both</strong> directions. Kind of like I&rsquo;m your friend and you are my friend. Note that the names used in the search pattern are just labels and do not represent the actual data in the GraphFrame.</p>\n<p>Note, motif searching is a resource intensive operation. Glue dev endpoints are not typically provisioned with much capacity so expect this task to take a little bit longer.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381763_1389512667","id":"20171124-013523_1619682681","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5086"},{"text":"## Due to the fact that graph operations are resource intensive and the dev endpoint is configured for data exploration and not heavy operations,\n## we'll take our dataset and split it into a large sample and a smaller sample so we can show how these graph operations work.\n\nverx_lg, verx_sm = venues_ratings.randomSplit([0.8, 0.2], seed=12345)","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381763_1389512667","id":"20171124-041416_684492035","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5087"},{"text":"motifs = graph.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\nmotifs.show(20, truncate=False)","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381763_1389512667","id":"20171123-160003_1926200373","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5088"},{"text":"%md\nSo what does the example above tell us? Looking at column **e** we see that users two particular users are connected to each other.\nWe also see the different venues they visited and the ratings they each gave.  Feel free to expriment with motif searching to identify more patterns.\n\nTry finding users that are connected to other users but they are not connected back.  i.e. I'm your friend but you are not my friend.","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>So what does the example above tell us? Looking at column <strong>e</strong> we see that users two particular users are connected to each other.<br/>We also see the different venues they visited and the ratings they each gave. Feel free to expriment with motif searching to identify more patterns.</p>\n<p>Try finding users that are connected to other users but they are not connected back. i.e. I&rsquo;m your friend but you are not my friend.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381763_1389512667","id":"20171124-015144_2001889716","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5089"},{"text":"%md\n# PageRank\n\n[PageRank](https://en.wikipedia.org/wiki/PageRank) is a link analysis algorithm used to assign numerical weights to each edge in the graph with the purpose of measuring the connected node's relative importance.\n\nThe GraphFrames [API](https://graphframes.github.io/user-guide.html#pagerank) supports both non-personalized and personalized PageRank.  Can you figure out how to run PageRank for a specific user ID ?","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>PageRank</h1>\n<p><a href=\"https://en.wikipedia.org/wiki/PageRank\">PageRank</a> is a link analysis algorithm used to assign numerical weights to each edge in the graph with the purpose of measuring the connected node&rsquo;s relative importance.</p>\n<p>The GraphFrames <a href=\"https://graphframes.github.io/user-guide.html#pagerank\">API</a> supports both non-personalized and personalized PageRank. Can you figure out how to run PageRank for a specific user ID ?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381763_1389512667","id":"20171124-031119_1799487122","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5090"},{"text":"g = GraphFrame(df_users, df_connections)\nresults = g.pageRank(resetProbability=0.15, maxIter=5)\nresults.vertices.select(\"id\", \"pagerank\").show(10)\nresults.edges.select(\"src\", \"dst\", \"weight\").show(10)","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381764_1387588923","id":"20171124-030956_922651571","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5091"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511793114514_-708324992","id":"20171127-143154_1146924186","dateCreated":"2017-11-27T14:31:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6618","text":"%md\n\nComplete the code to perform PageRank on a specific user ID.  You should first verify the user ID is in the dataframe. Here is a [hint](https://graphframes.github.io/user-guide.html#pagerank)","dateUpdated":"2017-11-27T14:33:06+0000","dateFinished":"2017-11-27T14:33:06+0000","dateStarted":"2017-11-27T14:33:06+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Complete the code to perform PageRank on a specific user ID. You should first verify the user ID is in the dataframe. Here is a <a href=\"https://graphframes.github.io/user-guide.html#pagerank\">hint</a></p>\n</div>"}]}},{"text":"g = GraphFrame(df_users, df_connections)\n\n## Complete the code","dateUpdated":"2017-11-27T14:19:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1511792381764_1387588923","id":"20171124-043014_1235234825","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5092"},{"text":"%md\n# Label Propagation Algorithm (LPA)\n\nRun [LPA](https://graphframes.github.io/user-guide.html#label-propagation-algorithm-lpa) on the graph to do community detection.\n\nNote, this operation is resource intensive and will take a long time on the dev endpoint.","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Label Propagation Algorithm (LPA)</h1>\n<p>Run <a href=\"https://graphframes.github.io/user-guide.html#label-propagation-algorithm-lpa\">LPA</a> on the graph to do community detection.</p>\n<p>Note, this operation is resource intensive and will take a long time on the dev endpoint.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511792381764_1387588923","id":"20171124-031132_514941540","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5093"},{"text":"g = GraphFrame(df_users, df_connections)\n\n## Complete the code to do LPA on variable g\nresult = g.labelPropagation(maxIter=5)\nresult.select(\"id\", \"label\").show()","dateUpdated":"2017-11-27T14:19:41+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381764_1387588923","id":"20171124-031130_1693051724","dateCreated":"2017-11-27T14:19:41+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5094"},{"dateUpdated":"2017-11-27T14:33:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511792381765_1387204174","id":"20171124-034902_1810624129","dateCreated":"2017-11-27T14:19:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:5095","text":"%md\n# Finished!\n\nYou are now finished with this module.  Thank you","user":"admin","dateFinished":"2017-11-27T14:33:43+0000","dateStarted":"2017-11-27T14:33:43+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Finished!</h1>\n<p>You are now finished with this module. Thank you</p>\n</div>"}]}},{"text":"%md\n","user":"admin","dateUpdated":"2017-11-27T14:33:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511793222989_-1149262555","id":"20171127-143342_1644789966","dateCreated":"2017-11-27T14:33:42+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6692"}],"name":"Workshop 3","id":"2D2HGQ8TB","angularObjects":{"2CRE5X63N:shared_process":[],"2CQZ9JC8F:shared_process":[],"2CNAPD13W:shared_process":[],"2CRC324G8:existing_process":[],"2CQ1X8PFU:shared_process":[],"2CNFC7TNE:shared_process":[],"2CMQ2EHTE:shared_process":[],"2CQUCJVJA:shared_process":[],"2CQY3SUDK:shared_process":[],"2CNA37GFQ:shared_process":[],"2CPH9H73W:shared_process":[],"2CR2XFEE3:shared_process":[],"2CQ9YVF7W:shared_process":[],"2CP1P36EU:shared_process":[],"2CMR9MKCE:shared_process":[],"2CR2YZBFW:shared_process":[],"2CPJK4U6W:shared_process":[],"2CR2Z9UR2:shared_process":[],"2CPQWHWM3:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}